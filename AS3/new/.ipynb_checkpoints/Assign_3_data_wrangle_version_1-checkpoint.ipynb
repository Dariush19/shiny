{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Title: COVID-19 Data Visualisation - Data Wrangling\n",
    "#### Author: Dariush Riazati"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Revisiones\n",
    "- Set increment to zero if today's figure was revised down compared to yesterday.\n",
    "- Rest null with 0 after left join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and install necessary Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import sys\n",
    "#!{sys.executable} -m pip install plotly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Generate consistent column names for each day that could be later used as valid dates. \n",
    "The column names for dates is used for all data sets.\n",
    "'''\n",
    "start_date = datetime.datetime(2020, 1, 22)\n",
    "end_date = datetime.datetime(2020, 4, 13)\n",
    "oneDay = datetime.timedelta(days=1)\n",
    "\n",
    "# Generate column names for days to cover\n",
    "date_col_names = []\n",
    "while start_date <= end_date:\n",
    "    col_name = \"{}-{}-{}\".format(start_date.day, start_date.month, start_date.year)\n",
    "    start_date += oneDay\n",
    "    date_col_names.append(col_name)\n",
    "    \n",
    "all_columns = ['State', 'Country', 'Lat', 'Long'] + date_col_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-usable functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Print columns with null value.\n",
    "'''\n",
    "def get_cols_with_null(df):\n",
    "    # Get a list containing each column and number of null values in each column\n",
    "    col_list = []\n",
    "    x = len(df) - df.count()\n",
    "    for i in range(0, len(all_columns)):\n",
    "        if x[i] > 0:\n",
    "            col_list.append(str(all_columns[i]))\n",
    "            \n",
    "    return col_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Transpose the count values for each of the day columns into a a 3 column dataframe containing the id, day value and the count for the day.\n",
    "'''\n",
    "def trans_data(input_df, date_col_names):\n",
    "    col_id =   []\n",
    "    col_date = []\n",
    "    col_cnt =  []\n",
    "    col_acc_cnt =  []\n",
    "    ncol = len(date_col_names)\n",
    "    for i in range(0, len(input_df)):\n",
    "        for j in range(0, ncol):\n",
    "          col_id.append(input_df.iloc[i][len(date_col_names)])\n",
    "          col_date.append(date_col_names[j])\n",
    "          col_acc_cnt.append(input_df.iloc[i][j])\n",
    "          offset = j + len(date_col_names) + 1\n",
    "          col_cnt.append(input_df.iloc[i][offset])\n",
    "\n",
    "    the_dict = {'id':col_id,'the_date':col_date, 'acc_count':col_acc_cnt, 'inc_count':col_cnt}\n",
    "    out_df = pd.DataFrame(the_dict) \n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Rename the columns as we want to make room for same column names to hold the increments. \n",
    "Figures/counts as given are accummulative. \n",
    "We need increments to be able to perform aggregation at the time of visualization.\n",
    "'''\n",
    "def get_increments(in_df, date_col_names):\n",
    "    for ix2 in range(0, len(date_col_names)): \n",
    "        col_name = 'X-' + date_col_names[ix2]\n",
    "        in_df[col_name] = 0\n",
    "\n",
    "    for ix1 in range(0, len(in_df)):\n",
    "        for ix2 in range(0, len(date_col_names)-1): \n",
    "            new_col = 'X-' + date_col_names[ix2+1]\n",
    "            delta = in_df.iloc[ix1][ix2+1] - in_df.iloc[ix1][ix2]\n",
    "            new_col_no = len(date_col_names) + ix2\n",
    "            if delta < 0:\n",
    "                delta = 0\n",
    "            in_df.loc[ix1].at[new_col] = delta\n",
    "            \n",
    "    return in_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read COVID-19 data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = pd.read_csv(\"time_series_covid19_confirmed_global.csv\")\n",
    "d1 = pd.read_csv(\"time_series_covid19_deaths_global.csv\")\n",
    "r1 = pd.read_csv(\"time_series_covid19_recovered_global.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Confirmed data set: Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before update: Names of columns with null values:  ['State']\n",
      "After update: Names of columns with null values:  []\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "1. Rename date columns\n",
    "2. identify columns with a null vale.\n",
    "3. State happens to be the only one; Replace its missing values with the name of the country;\n",
    "4. Add an 'id' column with a sequential number (equalling the index.\n",
    "'''\n",
    "c1.columns = all_columns\n",
    "print(\"Before update: Names of columns with null values: \", get_cols_with_null(c1))\n",
    "c1.State.fillna(c1.Country, inplace=True)\n",
    "print(\"After update: Names of columns with null values: \", get_cols_with_null(c1))\n",
    "c1['id'] = c1.index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['State', 'Country', 'Lat', 'Long', '22-1-2020', '23-1-2020',\n",
       "       '24-1-2020', '25-1-2020', '26-1-2020', '27-1-2020', '28-1-2020',\n",
       "       '29-1-2020', '30-1-2020', '31-1-2020', '1-2-2020', '2-2-2020',\n",
       "       '3-2-2020', '4-2-2020', '5-2-2020', '6-2-2020', '7-2-2020', '8-2-2020',\n",
       "       '9-2-2020', '10-2-2020', '11-2-2020', '12-2-2020', '13-2-2020',\n",
       "       '14-2-2020', '15-2-2020', '16-2-2020', '17-2-2020', '18-2-2020',\n",
       "       '19-2-2020', '20-2-2020', '21-2-2020', '22-2-2020', '23-2-2020',\n",
       "       '24-2-2020', '25-2-2020', '26-2-2020', '27-2-2020', '28-2-2020',\n",
       "       '29-2-2020', '1-3-2020', '2-3-2020', '3-3-2020', '4-3-2020', '5-3-2020',\n",
       "       '6-3-2020', '7-3-2020', '8-3-2020', '9-3-2020', '10-3-2020',\n",
       "       '11-3-2020', '12-3-2020', '13-3-2020', '14-3-2020', '15-3-2020',\n",
       "       '16-3-2020', '17-3-2020', '18-3-2020', '19-3-2020', '20-3-2020',\n",
       "       '21-3-2020', '22-3-2020', '23-3-2020', '24-3-2020', '25-3-2020',\n",
       "       '26-3-2020', '27-3-2020', '28-3-2020', '29-3-2020', '30-3-2020',\n",
       "       '31-3-2020', '1-4-2020', '2-4-2020', '3-4-2020', '4-4-2020', '5-4-2020',\n",
       "       '6-4-2020', '7-4-2020', '8-4-2020', '9-4-2020', '10-4-2020',\n",
       "       '11-4-2020', '12-4-2020', '13-4-2020', 'id'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "5. Create two subsets of the list. \n",
    "One that includes all columns to and including longitude, \n",
    "and another that includes from the first date column to id.\n",
    "6. Add increments to accumulative figures.\n",
    "'''\n",
    "c11 = pd.concat((c1.loc[:, :'Long'], c1.loc[:, 'id']), axis=1)\n",
    "c12 = c1.loc[:, '22-1-2020':'id']\n",
    "\n",
    "c12 = get_increments(c12, date_col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "7. Transpose the dataframe around dates.\n",
    "8. Rename generic column names to specific column names\n",
    "9. (Left) Join the subset from step 5 with the transposed dates to get the full data set.\n",
    "'''\n",
    "c12_frame = trans_data(c12, date_col_names)\n",
    "c12_frame = c12_frame.rename(columns={'the_date': 'confirmed_date', 'acc_count': 'confirmed_acc_count', 'inc_count': 'confirmed_inc_count'})\n",
    "df_confirmed = pd.merge(c11, c12_frame, on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_confirmed.to_csv (r'E:\\Personal_Files\\Dariush\\monash\\visual\\AS3\\\\new\\df_confirmed.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: The number of rows in the resulting data frame matches the number of rows in the original data frame by the number of data\n"
     ]
    }
   ],
   "source": [
    "if len(df_confirmed) == len(c1) * (len(date_col_names) ):\n",
    "   print(\"Success: The number of rows in the resulting data frame matches the number of rows \\\n",
    "in the original data frame by the number of data\")\n",
    "else:\n",
    "   print(\"Something wrong: The number of rows in the resulting data frame does not matches with \\\n",
    "number of rows in the original data frame by the number of data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform same data wrangling for Deaths data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before update: Names of columns with null values:  ['State']\n",
      "After update: Names of columns with null values:  []\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "1. Rename date columns\n",
    "2. identify columns with a null vale.\n",
    "3. State happens to be the only one; Replace its missing values with the name of the country;\n",
    "4. Add an 'id' column with a sequential number (equalling the index.\n",
    "'''\n",
    "d1.columns = all_columns\n",
    "print(\"Before update: Names of columns with null values: \", get_cols_with_null(d1))\n",
    "d1.State.fillna(d1.Country, inplace=True)\n",
    "print(\"After update: Names of columns with null values: \", get_cols_with_null(d1))\n",
    "d1['id'] = d1.index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "5. Create two subsets of the list. \n",
    "   One that includes all columns to and including longitude, \n",
    "   and another that includes from the first date column to id.\n",
    "6. Add increments to accumulative figures.\n",
    "'''\n",
    "d11 = pd.concat((d1.loc[:, :'Long'], d1.loc[:, 'id']), axis=1)\n",
    "d12 = d1.loc[:, '22-1-2020':'id']\n",
    "\n",
    "d12 = get_increments(d12, date_col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: The number of rows in the resulting (Deaths) data frame matches the number of rows in the original data frame by the number of data\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "7. Transpose the dataframe around dates.\n",
    "8. Rename generic column names to specific column names\n",
    "9. (Left) Join the subset from step 5 with the transposed dates to get the full data set.\n",
    "'''\n",
    "d12_frame = trans_data(d12, date_col_names)\n",
    "d12_frame = d12_frame.rename(columns={'the_date': 'death_date', 'acc_count': 'death_acc_count', 'inc_count': 'death_inc_count'})\n",
    "\n",
    "df_deaths = pd.merge(d11, d12_frame, on='id')\n",
    "\n",
    "if len(df_deaths) == len(d1) * len(date_col_names):\n",
    "   print(\"Success: The number of rows in the resulting (Deaths) data frame matches the number of rows \\\n",
    "in the original data frame by the number of data\")\n",
    "else:\n",
    "   print(\"Something wrong: The number of rows in the resulting (Deaths) data frame does not matches with \\\n",
    "number of rows in the original data frame by the number of data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport plotly.express as px\\n\\nfig = px.bar(df_deaths, x='death_date', y='death_count',\\n              color='death_date',\\n             labels={'Confirmed Cases':'Corona Stats'}, height=400)\\nfig.show()\\n\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import plotly.express as px\n",
    "\n",
    "fig = px.bar(df_deaths, x='death_date', y='death_count',\n",
    "              color='death_date',\n",
    "             labels={'Confirmed Cases':'Corona Stats'}, height=400)\n",
    "fig.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform same data wrangling process for recovered data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before update: Names of columns with null values:  ['State']\n",
      "After update: Names of columns with null values:  []\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "1. Rename date columns\n",
    "2. identify columns with a null vale.\n",
    "3. State happens to be the only one; Replace its missing values with the name of the country;\n",
    "4. Add an 'id' column with a sequential number (equalling the index.\n",
    "'''\n",
    "r1.columns = all_columns\n",
    "print(\"Before update: Names of columns with null values: \", get_cols_with_null(r1))\n",
    "r1.State.fillna(r1.Country, inplace=True)\n",
    "print(\"After update: Names of columns with null values: \", get_cols_with_null(r1))\n",
    "r1['id'] = r1.index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "5. Create two subsets of the list. \n",
    "   One that includes all columns to and including longitude, \n",
    "   and another that includes from the first date column to id.\n",
    "6. Add increments to accumulative figures.\n",
    "'''\n",
    "r11 = pd.concat((r1.loc[:, :'Long'], r1.loc[:, 'id']), axis=1)\n",
    "r12 = r1.loc[:, '22-1-2020':'id']\n",
    "\n",
    "r12 = get_increments(r12, date_col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: The number of rows in the resulting (Recovered) data frame matches the number of rows in the original data frame by the number of data\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "7. Transpose the dataframe around dates.\n",
    "8. Rename generic column names to specific column names\n",
    "9. (Left) Join the subset from step 5 with the transposed dates to get the full data set.\n",
    "'''\n",
    "r12_frame = trans_data(r12, date_col_names)\n",
    "r12_frame = r12_frame.rename(columns={'the_date': 'recovered_date', 'acc_count': 'recovered_acc_count', 'inc_count': 'recovered_inc_count'})\n",
    "\n",
    "df_recovered = pd.merge(r11, r12_frame, on='id')\n",
    "\n",
    "if len(df_recovered) == len(r1) * len(date_col_names):\n",
    "   print(\"Success: The number of rows in the resulting (Recovered) data frame matches the number of rows \\\n",
    "in the original data frame by the number of data\")\n",
    "else:\n",
    "   print(\"Something wrong: The number of rows in the resulting (Recovered) data frame does not matches with \\\n",
    "number of rows in the original data frame by the number of data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recovered.to_csv (r'E:\\Personal_Files\\Dariush\\monash\\visual\\AS3\\\\new\\df_recovered.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport plotly.express as px\\n\\nfig = px.bar(df_recovered, x='recovered_date', y='recovered_inc_count',\\n              color='recovered_date',\\n             labels={'Recovered Cases':'Corona Stats'}, height=400)\\nfig.show()\\n\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import plotly.express as px\n",
    "\n",
    "fig = px.bar(df_recovered, x='recovered_date', y='recovered_inc_count',\n",
    "              color='recovered_date',\n",
    "             labels={'Recovered Cases':'Corona Stats'}, height=400)\n",
    "fig.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join all three data sets into one data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_confirmed_death = pd.merge(df_confirmed, df_deaths,  how='outer', left_on=['Country', 'State', 'confirmed_date'], right_on=['Country', 'State', 'death_date'])\n",
    "df_confirmed_death_recovered = pd.merge(df_confirmed_death, df_recovered, how='outer', left_on=['Country', 'State', 'confirmed_date'], right_on=['Country', 'State', 'recovered_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_confirmed_death_recovered[['Country', 'State', 'Long', 'Lat', 'confirmed_date', 'confirmed_acc_count', 'confirmed_inc_count', 'death_acc_count', 'death_inc_count', 'recovered_acc_count', 'recovered_inc_count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Remove anu null value resulting from left join.\n",
    "'''\n",
    "df_final_2 = df_final.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_2.to_csv (r'E:\\Personal_Files\\Dariush\\monash\\visual\\AS3\\\\new\\time_series_covid19_ALL_global.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
